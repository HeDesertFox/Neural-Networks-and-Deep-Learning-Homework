{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自监督学习\n",
    "框架：MoCo\n",
    "数据集：mini-Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heyh0\\.conda\\envs\\deeplearning39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\heyh0\\.conda\\envs\\deeplearning39\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\heyh0\\.conda\\envs\\deeplearning39\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from data_preparation import get_dataloaders\n",
    "from model import get_resnet18_model\n",
    "from pretraining import MoCo, train_moco\n",
    "from training_finetuning import train_cifar100, validate_cifar100, finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "batch_size = 128\n",
    "num_epochs_pretrain = 200\n",
    "num_epochs_finetune = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据加载器\n",
    "mini_imagenet_loader, train_loader_cifar100, test_loader_cifar100 = get_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoCo 预训练\n",
    "print(\"Starting MoCo pre-training on mini-ImageNet...\")\n",
    "moco_model = MoCo(base_encoder=get_resnet18_model, dim=128, K=65536, m=0.999, T=0.07, mlp=False).cuda()\n",
    "moco_optimizer = optim.SGD(moco_model.parameters(), lr=0.03, momentum=0.9, weight_decay=1e-4)\n",
    "moco_criterion = nn.CrossEntropyLoss().cuda()\n",
    "moco_writer = SummaryWriter(log_dir='logs/moco')\n",
    "\n",
    "for epoch in range(num_epochs_pretrain):\n",
    "    train_loss = train_moco(mini_imagenet_loader, moco_model, moco_criterion, moco_optimizer, epoch, moco_writer)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_pretrain}], MoCo Loss: {train_loss:.4f}')\n",
    "moco_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 MoCo 预训练的 encoder_q\n",
    "moco_pretrained_model = moco_model.encoder_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微调和评估 MoCo 预训练模型\n",
    "print(\"Finetuning MoCo pre-trained model on CIFAR-100...\")\n",
    "best_lr = finetune(train_loader_cifar100, test_loader_cifar100, moco_pretrained_model, learning_rate_list, num_epochs_finetune)\n",
    "print(f'Best learning rate for fine-tuning: {best_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最佳学习率训练和评估 MoCo 预训练模型\n",
    "print(\"Training MoCo pre-trained model on CIFAR-100 with best learning rate...\")\n",
    "finetune_optimizer = optim.Adam([\n",
    "    {'params': moco_pretrained_model.parameters(), 'lr': best_lr / 10},\n",
    "    {'params': moco_pretrained_model.fc.parameters(), 'lr': best_lr}\n",
    "])\n",
    "finetune_writer = SummaryWriter(log_dir='logs/moco_finetune')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs_finetune):\n",
    "    train_loss, train_acc = train_cifar100(train_loader_cifar100, moco_pretrained_model, criterion, finetune_optimizer, epoch, finetune_writer)\n",
    "    val_loss, val_acc = validate_cifar100(test_loader_cifar100, moco_pretrained_model, criterion, epoch, finetune_writer)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_finetune}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "finetune_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 ImageNet 预训练模型\n",
    "print(\"Evaluating ImageNet pre-trained model on CIFAR-100...\")\n",
    "imagenet_pretrained_model = get_resnet18_model(pretrained=True, num_classes=100).cuda()\n",
    "imagenet_best_lr = finetune(train_loader_cifar100, test_loader_cifar100, imagenet_pretrained_model, learning_rate_list, num_epochs_finetune)\n",
    "\n",
    "# 使用最佳学习率训练和评估 ImageNet 预训练模型\n",
    "finetune_optimizer = optim.Adam([\n",
    "    {'params': imagenet_pretrained_model.parameters(), 'lr': imagenet_best_lr / 10},\n",
    "    {'params': imagenet_pretrained_model.fc.parameters(), 'lr': imagenet_best_lr}\n",
    "])\n",
    "imagenet_finetune_writer = SummaryWriter(log_dir='logs/imagenet_finetune')\n",
    "\n",
    "for epoch in range(num_epochs_finetune):\n",
    "    train_loss, train_acc = train_cifar100(train_loader_cifar100, imagenet_pretrained_model, criterion, finetune_optimizer, epoch, imagenet_finetune_writer)\n",
    "    val_loss, val_acc = validate_cifar100(test_loader_cifar100, imagenet_pretrained_model, criterion, epoch, imagenet_finetune_writer)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_finetune}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "imagenet_finetune_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和评估随机初始化模型\n",
    "print(\"Training randomly initialized model on CIFAR-100...\")\n",
    "random_init_model = get_resnet18_model(pretrained=False, num_classes=100).cuda()\n",
    "random_best_lr = finetune(train_loader_cifar100, test_loader_cifar100, random_init_model, learning_rate_list, num_epochs_finetune)\n",
    "\n",
    "# 使用最佳学习率训练和评估随机初始化模型\n",
    "finetune_optimizer = optim.Adam([\n",
    "    {'params': random_init_model.parameters(), 'lr': random_best_lr / 10},\n",
    "    {'params': random_init_model.fc.parameters(), 'lr': random_best_lr}\n",
    "])\n",
    "random_finetune_writer = SummaryWriter(log_dir='logs/random_finetune')\n",
    "\n",
    "for epoch in range(num_epochs_finetune):\n",
    "    train_loss, train_acc = train_cifar100(train_loader_cifar100, random_init_model, criterion, finetune_optimizer, epoch, random_finetune_writer)\n",
    "    val_loss, val_acc = validate_cifar100(test_loader_cifar100, random_init_model, criterion, epoch, random_finetune_writer)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_finetune}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "random_finetune_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
